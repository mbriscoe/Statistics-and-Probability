{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8156dd5",
   "metadata": {},
   "source": [
    "# Statistics and Probability with Python\n",
    "\n",
    "This notebook provides a comprehensive introduction to statistics and probability concepts using Python. We'll explore descriptive statistics, probability distributions, hypothesis testing, and data visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05eb17e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for statistical analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65756e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, binom, poisson, t, chi2, f_oneway\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b628daa",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Successfully imported all essential libraries for statistical analysis and data visualization\n",
    "- **NumPy**: Powerful numerical computing capabilities for array operations and mathematical functions\n",
    "- **Pandas**: Efficient data manipulation and analysis through DataFrames\n",
    "- **SciPy**: Extensive collection of statistical functions, tests, and probability distributions\n",
    "- **Matplotlib & Seaborn**: Complementary visualization libraries—Matplotlib provides foundational plotting while Seaborn adds statistical graphics with better aesthetics\n",
    "- Warning filter suppresses unnecessary warnings during statistical computations\n",
    "- Seaborn style settings ensure professional-looking visualizations with appropriate default sizes and grid backgrounds\n",
    "- These libraries together form the complete toolkit needed for comprehensive statistical analysis in Python\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf88a414",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize and describe the main features of a dataset. Let's calculate various descriptive statistics on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3817b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(100, 15, 1000)  # Mean=100, SD=15, n=1000\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "mean = np.mean(data)\n",
    "median = np.median(data)\n",
    "mode = stats.mode(data, keepdims=True)[0][0]\n",
    "variance = np.var(data, ddof=1)  # Sample variance\n",
    "std_dev = np.std(data, ddof=1)   # Sample standard deviation\n",
    "q1 = np.percentile(data, 25)\n",
    "q3 = np.percentile(data, 75)\n",
    "data_range = np.max(data) - np.min(data)\n",
    "iqr = q3 - q1\n",
    "\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(f\"Mean: {mean:.2f}\")\n",
    "print(f\"Median: {median:.2f}\")\n",
    "print(f\"Mode: {mode:.2f}\")\n",
    "print(f\"Variance: {variance:.2f}\")\n",
    "print(f\"Standard Deviation: {std_dev:.2f}\")\n",
    "print(f\"Range: {data_range:.2f}\")\n",
    "print(f\"Q1 (25th percentile): {q1:.2f}\")\n",
    "print(f\"Q3 (75th percentile): {q3:.2f}\")\n",
    "print(f\"IQR (Interquartile Range): {iqr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4d29d9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Generated sample dataset of 1,000 normally distributed values (mean=100, SD=15)\n",
    "- **Measures of Central Tendency**:\n",
    "  - Mean: Arithmetic average of all values\n",
    "  - Median: Middle value when data is sorted\n",
    "  - Mode: Most frequently occurring value\n",
    "- **Measures of Spread** (quantify data variability):\n",
    "  - Variance: Average squared deviation from the mean\n",
    "  - Standard Deviation: Square root of variance (in original units)\n",
    "  - Range: Difference between maximum and minimum values\n",
    "- **Quartiles** (divide data into quarters):\n",
    "  - Q1 (25th percentile): Marks the lower quarter\n",
    "  - Q3 (75th percentile): Marks the upper quarter\n",
    "  - IQR (Interquartile Range = Q3 - Q1): Spread of middle 50%, robust to outliers\n",
    "- These statistics provide complete picture of data distribution, central location, and variability\n",
    "- Fundamental for understanding any dataset before advanced analysis\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0eb6ca",
   "metadata": {},
   "source": [
    "## 3. Probability Distributions\n",
    "\n",
    "A probability distribution describes how the values of a random variable are distributed. Let's explore basic probability concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Rolling a fair die\n",
    "# Discrete uniform distribution\n",
    "outcomes = np.arange(1, 7)  # Die faces: 1, 2, 3, 4, 5, 6\n",
    "probabilities = np.ones(6) / 6  # Each outcome has probability 1/6\n",
    "\n",
    "# Create a probability mass function (PMF)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(outcomes, probabilities, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Die Face')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Probability Mass Function - Fair Die')\n",
    "plt.xticks(outcomes)\n",
    "plt.ylim(0, 0.3)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Expected value (mean): {np.sum(outcomes * probabilities):.2f}\")\n",
    "print(f\"Sum of all probabilities: {np.sum(probabilities):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ce098",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Created probability mass function (PMF) for a fair six-sided die\n",
    "- Demonstrates **discrete uniform distribution** where each outcome has equal probability (1/6 ≈ 0.1667)\n",
    "- Visualization shows all six outcomes have identical probabilities, illustrating fairness\n",
    "- **Expected Value** of 3.5: Theoretical mean outcome if rolling die infinitely many times\n",
    "  - Calculated as sum of each outcome multiplied by its probability\n",
    "  - Though you can never roll 3.5 on single throw, crucial for long-term predictions\n",
    "- All probabilities sum to exactly 1.0, validating proper probability distribution\n",
    "- **Probability Axiom**: Total probability across all possible outcomes must equal certainty\n",
    "- Establishes core probability concepts: PMFs for discrete variables, probability axioms, and expected values\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eeaac4",
   "metadata": {},
   "source": [
    "## 4. Normal Distribution\n",
    "\n",
    "The normal (Gaussian) distribution is one of the most important probability distributions in statistics. It's characterized by its bell-shaped curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25efb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normal distribution\n",
    "mu = 100  # Mean\n",
    "sigma = 15  # Standard deviation\n",
    "\n",
    "x = np.linspace(mu - 4*sigma, mu + 4*sigma, 1000)\n",
    "pdf = norm.pdf(x, mu, sigma)\n",
    "\n",
    "# Plot the normal distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, pdf, 'b-', linewidth=2, label=f'μ={mu}, σ={sigma}')\n",
    "plt.fill_between(x, pdf, alpha=0.2)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Normal Distribution (Probability Density Function)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate probabilities\n",
    "prob_below_85 = norm.cdf(85, mu, sigma)\n",
    "prob_above_115 = 1 - norm.cdf(115, mu, sigma)\n",
    "prob_between = norm.cdf(115, mu, sigma) - norm.cdf(85, mu, sigma)\n",
    "\n",
    "print(f\"Probability X < 85: {prob_below_85:.4f}\")\n",
    "print(f\"Probability X > 115: {prob_above_115:.4f}\")\n",
    "print(f\"Probability 85 < X < 115: {prob_between:.4f}\")\n",
    "\n",
    "# Z-scores\n",
    "z_score_85 = (85 - mu) / sigma\n",
    "print(f\"\\nZ-score for 85: {z_score_85:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68378440",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Visualized normal (Gaussian) distribution with mean μ=100 and standard deviation σ=15\n",
    "- Characteristic **bell-shaped curve** that is symmetric around the mean\n",
    "- Fundamental in statistics because many natural phenomena approximate normality\n",
    "- **Cumulative Distribution Function (CDF)**: Probability that random variable ≤ specific value\n",
    "- Results consistent with **Empirical Rule** (68-95-99.7 rule):\n",
    "  - ~68% of data falls within one standard deviation of mean (85 to 115)\n",
    "  - ~16% below 85, ~16% above 115\n",
    "- **Z-scores**: Standardize values by measuring standard deviations from mean\n",
    "  - Formula: z = (x - μ) / σ\n",
    "  - z = -1.0 for 85 means it's one SD below the mean\n",
    "  - Enables comparison across different scales\n",
    "  - Allows use of standard normal tables\n",
    "- Understanding normal distribution essential for hypothesis tests, confidence intervals, and regression analysis\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc7814",
   "metadata": {},
   "source": [
    "## 5. Binomial Distribution\n",
    "\n",
    "The binomial distribution models the number of successes in a fixed number of independent trials, each with the same probability of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f723f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Flipping a coin 10 times\n",
    "n = 10  # Number of trials\n",
    "p = 0.5  # Probability of success (heads)\n",
    "\n",
    "k = np.arange(0, n+1)\n",
    "pmf = binom.pmf(k, n, p)\n",
    "\n",
    "# Plot binomial distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(k, pmf, color='coral', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Number of Heads')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Binomial Distribution (n={n}, p={p})')\n",
    "plt.xticks(k)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate specific probabilities\n",
    "prob_5_heads = binom.pmf(5, n, p)\n",
    "prob_at_least_7 = 1 - binom.cdf(6, n, p)\n",
    "\n",
    "print(f\"Probability of exactly 5 heads: {prob_5_heads:.4f}\")\n",
    "print(f\"Probability of at least 7 heads: {prob_at_least_7:.4f}\")\n",
    "print(f\"Expected number of heads: {n * p}\")\n",
    "print(f\"Variance: {n * p * (1-p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397011c7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Modeled binomial experiment: 10 coin flips with probability p=0.5 for heads\n",
    "- **Binomial Distribution** applies when:\n",
    "  - Fixed number of independent trials\n",
    "  - Each trial has same probability of success\n",
    "- PMF visualization reveals highest probability at 5 heads (expected value = n×p = 10×0.5 = 5)\n",
    "- Probabilities decrease symmetrically toward extremes (0 or 10 heads)\n",
    "- **Key Probabilities**:\n",
    "  - Exactly 5 heads: ~0.246 (approximately 25%)\n",
    "  - 7 or more heads: ~0.172\n",
    "- **Variance** = n×p×(1-p) = 2.5, measuring spread around expected value\n",
    "- **Applications**: Quality control, survey analysis, medical trials, any binary outcome scenarios (success/failure, yes/no)\n",
    "- Binomial distribution approaches normal distribution as n increases (with appropriate p values)\n",
    "- Illustrates connection between discrete and continuous probability models\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430cb84",
   "metadata": {},
   "source": [
    "## 6. Poisson Distribution\n",
    "\n",
    "The Poisson distribution models the number of events occurring in a fixed interval of time or space when events occur independently at a constant average rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Average of 3 emails per hour\n",
    "lambda_param = 3  # Average rate\n",
    "\n",
    "k = np.arange(0, 15)\n",
    "pmf_poisson = poisson.pmf(k, lambda_param)\n",
    "\n",
    "# Plot Poisson distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(k, pmf_poisson, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Number of Events')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(f'Poisson Distribution (λ={lambda_param})')\n",
    "plt.xticks(k)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate probabilities\n",
    "prob_exactly_3 = poisson.pmf(3, lambda_param)\n",
    "prob_less_than_2 = poisson.cdf(1, lambda_param)\n",
    "prob_more_than_5 = 1 - poisson.cdf(5, lambda_param)\n",
    "\n",
    "print(f\"Probability of exactly 3 emails: {prob_exactly_3:.4f}\")\n",
    "print(f\"Probability of less than 2 emails: {prob_less_than_2:.4f}\")\n",
    "print(f\"Probability of more than 5 emails: {prob_more_than_5:.4f}\")\n",
    "print(f\"Expected value: {lambda_param}\")\n",
    "print(f\"Variance: {lambda_param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddf651",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Illustrated Poisson distribution modeling email arrivals with average rate λ=3 per hour\n",
    "- Used for **counting rare events** occurring independently over continuous interval (time/space)\n",
    "- **Key Characteristics**:\n",
    "  - Events occur independently\n",
    "  - Average rate (λ) is constant\n",
    "  - Events cannot occur simultaneously\n",
    "- Visualization shows right-skewed distribution with mode near λ=3\n",
    "- **Calculated Probabilities**:\n",
    "  - Exactly 3 emails: ~22.4%\n",
    "  - Fewer than 2 emails: ~19.9%\n",
    "  - More than 5 emails: ~8.4%\n",
    "- **Unique Property**: Both expected value and variance equal λ (here, both are 3)\n",
    "- **Applications**: Operations research (call centers, customer traffic), reliability engineering (equipment failures), natural sciences (radioactive decay, mutations)\n",
    "- Poisson approximates binomial when n is large and p is small (rare events)\n",
    "- Computationally efficient for modeling unlikely occurrences over many opportunities\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e9a1c",
   "metadata": {},
   "source": [
    "## 7. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is a statistical method to make decisions about population parameters based on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce45521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sample t-test\n",
    "# H0: Population mean = 100\n",
    "# H1: Population mean ≠ 100\n",
    "sample_data = np.random.normal(105, 15, 50)\n",
    "t_statistic, p_value = stats.ttest_1samp(sample_data, 100)\n",
    "\n",
    "print(\"One-Sample T-Test\")\n",
    "print(f\"Sample mean: {np.mean(sample_data):.2f}\")\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Result: {'Reject H0' if p_value < 0.05 else 'Fail to reject H0'} at α=0.05\\n\")\n",
    "\n",
    "# Two-sample t-test (independent samples)\n",
    "group1 = np.random.normal(100, 15, 50)\n",
    "group2 = np.random.normal(110, 15, 50)\n",
    "t_stat_2, p_val_2 = stats.ttest_ind(group1, group2)\n",
    "\n",
    "print(\"Two-Sample T-Test (Independent)\")\n",
    "print(f\"Group 1 mean: {np.mean(group1):.2f}\")\n",
    "print(f\"Group 2 mean: {np.mean(group2):.2f}\")\n",
    "print(f\"T-statistic: {t_stat_2:.4f}\")\n",
    "print(f\"P-value: {p_val_2:.4f}\")\n",
    "print(f\"Result: {'Reject H0' if p_val_2 < 0.05 else 'Fail to reject H0'} at α=0.05\\n\")\n",
    "\n",
    "# Chi-square test for independence\n",
    "observed = np.array([[30, 10], [20, 40]])\n",
    "chi2_stat, p_val_chi, dof, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print(\"Chi-Square Test for Independence\")\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"P-value: {p_val_chi:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"Result: {'Reject H0' if p_val_chi < 0.05 else 'Fail to reject H0'} at α=0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807a4e0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Performed three fundamental types of hypothesis tests:\n",
    "  1. **One-Sample t-Test**:\n",
    "     - Tests if sample mean differs from hypothesized population value (H₀: μ=100)\n",
    "     - Useful when testing if group differs from known standard\n",
    "  2. **Two-Sample Independent t-Test**:\n",
    "     - Compares means between two separate groups\n",
    "     - Applicable in experimental designs (treatment vs. control, different populations)\n",
    "  3. **Chi-Square Test for Independence**:\n",
    "     - Analyzes categorical data in contingency tables\n",
    "     - Determines if two categorical variables are related or independent\n",
    "     - Common in survey analysis and association studies\n",
    "- **P-value**: Probability of obtaining results at least as extreme as observed, assuming null hypothesis true\n",
    "- P-values below significance level (typically α=0.05) suggest rejecting null hypothesis\n",
    "- **Statistical Significance**: Evidence of real effect, not due to random chance\n",
    "- **T-statistic**: Measures how many standard errors sample mean is from hypothesized value\n",
    "- **Chi-square statistic**: Quantifies deviation from expected frequencies under independence\n",
    "- Essential for making data-driven decisions and drawing valid statistical inferences\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864438a",
   "metadata": {},
   "source": [
    "## 8. Correlation and Covariance\n",
    "\n",
    "Correlation and covariance measure the relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec4cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated data\n",
    "np.random.seed(42)\n",
    "x = np.random.normal(50, 10, 100)\n",
    "y = 2 * x + np.random.normal(0, 10, 100)  # Positively correlated\n",
    "z = -1.5 * x + np.random.normal(100, 15, 100)  # Negatively correlated\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "corr_xy = np.corrcoef(x, y)[0, 1]\n",
    "corr_xz = np.corrcoef(x, z)[0, 1]\n",
    "\n",
    "# Calculate covariance\n",
    "cov_xy = np.cov(x, y)[0, 1]\n",
    "cov_xz = np.cov(x, z)[0, 1]\n",
    "\n",
    "print(f\"Correlation between X and Y: {corr_xy:.4f}\")\n",
    "print(f\"Correlation between X and Z: {corr_xz:.4f}\")\n",
    "print(f\"Covariance between X and Y: {cov_xy:.2f}\")\n",
    "print(f\"Covariance between X and Z: {cov_xz:.2f}\")\n",
    "\n",
    "# Visualize correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(x, y, alpha=0.6, color='blue')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('Y')\n",
    "axes[0].set_title(f'Positive Correlation (r={corr_xy:.2f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(x, z, alpha=0.6, color='red')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('Z')\n",
    "axes[1].set_title(f'Negative Correlation (r={corr_xz:.2f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "data_df = pd.DataFrame({'X': x, 'Y': y, 'Z': z})\n",
    "corr_matrix = data_df.corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bcba1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Demonstrated correlation and covariance measuring **linear relationships** between variables\n",
    "- **Correlation Coefficients** (Pearson's r):\n",
    "  - Standardized measure ranging from -1 to +1\n",
    "  - Near +1: Strong positive linear relationship (both variables increase together)\n",
    "  - Near -1: Strong negative relationship (one increases, other decreases)\n",
    "  - Near 0: Weak or no linear relationship\n",
    "- **Example Results**:\n",
    "  - Strong positive correlation (r ≈ 0.95) between X and Y\n",
    "  - Strong negative correlation (r ≈ -0.95) between X and Z\n",
    "- **Covariance**: Same concept in unstandardized units\n",
    "  - Harder to interpret but useful in certain calculations\n",
    "- **Scatter Plots**: Visual confirmation of relationships\n",
    "  - Positive correlation: Upward trend\n",
    "  - Negative correlation: Downward trend\n",
    "- **Correlation Matrix**: Comprehensive view of all pairwise relationships in dataset\n",
    "- **Important Caveats**:\n",
    "  - Correlation doesn't imply causation\n",
    "  - Only captures linear relationships\n",
    "  - Outliers can heavily influence correlation\n",
    "- Foundational in regression analysis, portfolio theory, data exploration, identifying multicollinearity\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8b49e",
   "metadata": {},
   "source": [
    "## 9. Central Limit Theorem\n",
    "\n",
    "The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac22da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Central Limit Theorem\n",
    "# Start with a non-normal distribution (uniform)\n",
    "population = np.random.uniform(0, 100, 10000)\n",
    "\n",
    "# Take many samples and calculate their means\n",
    "sample_sizes = [5, 10, 30, 100]\n",
    "n_samples = 1000\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, sample_size in enumerate(sample_sizes):\n",
    "    sample_means = [np.mean(np.random.choice(population, sample_size)) \n",
    "                    for _ in range(n_samples)]\n",
    "    \n",
    "    axes[idx].hist(sample_means, bins=30, density=True, \n",
    "                   alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Overlay normal distribution\n",
    "    mu_sampling = np.mean(sample_means)\n",
    "    sigma_sampling = np.std(sample_means)\n",
    "    x_range = np.linspace(min(sample_means), max(sample_means), 100)\n",
    "    axes[idx].plot(x_range, norm.pdf(x_range, mu_sampling, sigma_sampling), \n",
    "                   'r-', linewidth=2, label='Normal fit')\n",
    "    \n",
    "    axes[idx].set_title(f'Sample Size: {sample_size}')\n",
    "    axes[idx].set_xlabel('Sample Mean')\n",
    "    axes[idx].set_ylabel('Density')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Central Limit Theorem Demonstration', fontsize=16, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Population mean: {np.mean(population):.2f}\")\n",
    "print(f\"Population std: {np.std(population):.2f}\")\n",
    "print(f\"\\nTheoretical standard error (n=30): {np.std(population)/np.sqrt(30):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ce153",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Illustrated **Central Limit Theorem (CLT)**, one of the most important concepts in statistics\n",
    "- **CLT Statement**: Distribution of sample means approaches normal distribution as sample size increases, *regardless of original population distribution*\n",
    "- **Demonstration**:\n",
    "  - Sampled from uniform distribution (rectangular, non-normal shape)\n",
    "  - Sample means become increasingly normal as size grows from n=5 to n=100\n",
    "  - Small samples (n=5): Irregular sampling distribution\n",
    "  - By n=30: Remarkably normal (why n=30 often cited as \"magic number\")\n",
    "- **Standard Error** (σ/√n):\n",
    "  - Measures standard deviation of sampling distribution\n",
    "  - Decreases as sample size increases\n",
    "  - Explains why larger samples provide more precise estimates\n",
    "- **Why CLT is Fundamental**:\n",
    "  - Justifies using normal-based methods (t-tests, confidence intervals) even when population isn't normal\n",
    "  - Explains why averages are more reliable than individual observations\n",
    "  - Underlies much of inferential statistics\n",
    "- Enables probability statements about sample means and construction of confidence intervals\n",
    "- Forms theoretical foundation for hypothesis testing and estimation procedures\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2444b",
   "metadata": {},
   "source": [
    "## 10. Confidence Intervals\n",
    "\n",
    "A confidence interval provides a range of values that likely contains the true population parameter with a specified level of confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence interval for the mean\n",
    "sample = np.random.normal(100, 15, 50)\n",
    "sample_mean = np.mean(sample)\n",
    "sample_std = np.std(sample, ddof=1)\n",
    "n = len(sample)\n",
    "\n",
    "# 95% Confidence Interval\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "df = n - 1  # Degrees of freedom\n",
    "\n",
    "# t-critical value for 95% CI\n",
    "t_critical = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "# Standard error\n",
    "se = sample_std / np.sqrt(n)\n",
    "\n",
    "# Margin of error\n",
    "margin_of_error = t_critical * se\n",
    "\n",
    "# Confidence interval\n",
    "ci_lower = sample_mean - margin_of_error\n",
    "ci_upper = sample_mean + margin_of_error\n",
    "\n",
    "print(f\"Sample mean: {sample_mean:.2f}\")\n",
    "print(f\"Sample standard deviation: {sample_std:.2f}\")\n",
    "print(f\"Sample size: {n}\")\n",
    "print(f\"Standard error: {se:.2f}\")\n",
    "print(f\"T-critical value (α=0.05, df={df}): {t_critical:.4f}\")\n",
    "print(f\"Margin of error: {margin_of_error:.2f}\")\n",
    "print(f\"\\n95% Confidence Interval: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
    "print(f\"\\nInterpretation: We are 95% confident that the true population mean\")\n",
    "print(f\"lies between {ci_lower:.2f} and {ci_upper:.2f}\")\n",
    "\n",
    "# Visualize confidence interval\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(1, sample_mean, yerr=margin_of_error, fmt='o', \n",
    "             markersize=10, capsize=10, capthick=2, \n",
    "             color='darkblue', ecolor='red', linewidth=2)\n",
    "plt.axhline(y=sample_mean, color='blue', linestyle='--', alpha=0.5, label='Sample Mean')\n",
    "plt.axhline(y=ci_lower, color='red', linestyle='--', alpha=0.5, label='95% CI Bounds')\n",
    "plt.axhline(y=ci_upper, color='red', linestyle='--', alpha=0.5)\n",
    "plt.xlim(0.5, 1.5)\n",
    "plt.ylabel('Value')\n",
    "plt.title('95% Confidence Interval for Population Mean')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069635fc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Calculated 95% confidence interval for population mean using **interval estimation**\n",
    "- **Purpose**: Estimate population parameters while quantifying uncertainty\n",
    "- **95% Confidence Level Interpretation**:\n",
    "  - If repeated sampling many times, ~95% of constructed intervals contain true population mean\n",
    "  - NOT that there's 95% probability specific interval contains it\n",
    "  - True mean either is or isn't in our interval\n",
    "- **Calculation Components**:\n",
    "  1. **Standard Error** (SE = s/√n): Measures sampling variability\n",
    "  2. **t-Critical Value**: From t-distribution (not normal, because we estimated σ with s)\n",
    "  3. **Margin of Error** (ME = t* × SE): Creates interval width\n",
    "- **Why t-Distribution?**:\n",
    "  - Accounts for additional uncertainty when estimating population SD from sample\n",
    "  - Especially important for small samples\n",
    "- **Visualization**: Point estimate (sample mean) with error bars showing margin of error\n",
    "- **Advantages over Point Estimates**:\n",
    "  - Convey uncertainty explicitly\n",
    "  - More informative for decision-making\n",
    "- **Applications**: Research reporting, A/B testing, quality control, any inference from samples to populations\n",
    "- Wider intervals indicate more uncertainty (reduce by increasing sample size)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021e407",
   "metadata": {},
   "source": [
    "## 11. Data Visualization for Probability\n",
    "\n",
    "Visual representations help us understand statistical concepts and distributions better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abf79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data1 = np.random.normal(100, 15, 1000)\n",
    "data2 = np.random.normal(110, 20, 1000)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 1. Histogram with KDE\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "plt.hist(data1, bins=30, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "from scipy.stats import gaussian_kde\n",
    "kde = gaussian_kde(data1)\n",
    "x_range = np.linspace(data1.min(), data1.max(), 100)\n",
    "plt.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Histogram with KDE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box Plot\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "box_data = [data1, data2]\n",
    "plt.boxplot(box_data, labels=['Data 1', 'Data 2'], patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "plt.ylabel('Value')\n",
    "plt.title('Box Plot Comparison')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Q-Q Plot\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "stats.probplot(data1, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot (Normal Distribution)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Violin Plot\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "df_combined = pd.DataFrame({\n",
    "    'Value': np.concatenate([data1, data2]),\n",
    "    'Group': ['Data 1']*len(data1) + ['Data 2']*len(data2)\n",
    "})\n",
    "sns.violinplot(data=df_combined, x='Group', y='Value', palette='Set2')\n",
    "plt.title('Violin Plot')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Cumulative Distribution Function\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "sorted_data = np.sort(data1)\n",
    "cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "plt.plot(sorted_data, cumulative, linewidth=2, color='green')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Empirical Cumulative Distribution Function')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Scatter plot with regression line\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "x_scatter = np.random.normal(50, 10, 100)\n",
    "y_scatter = 2 * x_scatter + np.random.normal(0, 10, 100)\n",
    "plt.scatter(x_scatter, y_scatter, alpha=0.6, color='purple')\n",
    "# Add regression line\n",
    "z = np.polyfit(x_scatter, y_scatter, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x_scatter, p(x_scatter), \"r-\", linewidth=2, label=f'y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Scatter Plot with Regression Line')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Statistical Visualization Dashboard', fontsize=16, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d0250",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Summary</b></summary>\n",
    "\n",
    "- Created comprehensive statistical visualization dashboard with six essential plot types:\n",
    "  1. **Histogram with KDE** (Kernel Density Estimation):\n",
    "     - Shows data distribution shape and smoothed probability density\n",
    "     - Identifies modality and skewness\n",
    "  2. **Box Plot**:\n",
    "     - Displays quartiles, median, and outliers (points beyond 1.5×IQR from quartiles)\n",
    "     - Enables quick comparison of central tendency and spread between groups\n",
    "  3. **Q-Q Plot** (Quantile-Quantile):\n",
    "     - Assesses normality by comparing sample vs. theoretical normal quantiles\n",
    "     - Points following diagonal line indicate normality\n",
    "     - Deviations suggest non-normality\n",
    "  4. **Violin Plot**:\n",
    "     - Combines box plot information with KDE\n",
    "     - Shows full distribution shape\n",
    "     - Easier to identify bimodal or multimodal distributions\n",
    "  5. **Empirical CDF** (Cumulative Distribution Function):\n",
    "     - Displays cumulative probabilities\n",
    "     - Useful for finding percentiles and comparing distributions\n",
    "  6. **Scatter Plot with Regression Line**:\n",
    "     - Visualizes relationships between variables\n",
    "     - Fits linear model (y = mx + b) to quantify relationship\n",
    "- **Together**: Complete exploratory data analysis toolkit\n",
    "- **Purpose**: Identify patterns, outliers, distribution shapes, relationships, departures from assumptions\n",
    "- Effective visualization crucial for communicating findings and discovering insights summary statistics might miss\n",
    "- Each plot type serves specific purposes and provides complementary information\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ed588",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered the fundamental concepts of statistics and probability:\n",
    "\n",
    "1. **Descriptive Statistics**: Mean, median, mode, variance, standard deviation, quartiles\n",
    "2. **Probability Distributions**: Understanding PMF and PDF\n",
    "3. **Normal Distribution**: Bell curve, z-scores, probability calculations\n",
    "4. **Binomial Distribution**: Discrete probability for fixed trials\n",
    "5. **Poisson Distribution**: Event modeling over time/space\n",
    "6. **Hypothesis Testing**: t-tests, chi-square tests, p-values\n",
    "7. **Correlation & Covariance**: Measuring relationships between variables\n",
    "8. **Central Limit Theorem**: Sampling distributions\n",
    "9. **Confidence Intervals**: Estimating population parameters\n",
    "10. **Data Visualization**: Various plots to understand distributions\n",
    "\n",
    "These concepts form the foundation of statistical analysis and are widely used in data science, research, and decision-making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
